\section{ Описание моделей оценивания текста }\label{sec:models}

Перед обучением моделей корпус разбивается на независимые и гомогенные части: обучающую и тестовую выборки. Обучающая выборка используется для обучения модели, тестовая -- для проверки качества обучения и, собственно, оценки модели.

В работе рассматриваются следующие модели: 

\begin{itemize}
	\item $n$-граммные с фиксированным $n,\ n \in \{1,2,3\}$
	
	\item Backoff-модель, $n_{max} \in \{ 3, 5, 7 \}$
	
	\item Модель Катца (Katz),  $n_{max} \in \{ 3, 5, 7 \}$
\end{itemize}

Также из-за большого размера алфавита необходимо использовать сглаживание (smoothing) для учёта символов и $n$-грамм, не встретившихся в обучающей выборке. Подробнее о механизме сглаживания -- см. раздел \ref{sec:experiment}.

\subsection{ $N$-граммные модели с фиксированным $n$ }

\paragraph{ Обучение модели } Для данного $n$ по обучающей выборке собираются статистики по всем $n$-граммам. Эти статистики затем нормализуются и сериализуются для дальнейшего использования.

$$ C(x_{i - n + 1}, ..., x_{i - 1}, x_i) $$ 

\paragraph{ Применение модели } В силу простоты модели оценка $n$-граммы из тестовой выборки берётся напрямую из собранных на предыдущем этапе статистик.

$$ P(x_i | x_{i - n + 1}, ..., x_{i - 1}) = C(x_{i - n + 1}, ..., x_{i - 1}, x_i) $$

\subsection{ Backoff-модель } 

\paragraph{ Обучение модели } Этап обучения модели практически такой же, как и в случае простой $n$-граммной модели, с разницей в том, что здесь собираются статистики для всех $n \leq n_{max}$.

\paragraph{ Применение модели } Идея backoff-подхода состоит в том, что при нехватке данных для оценки какой-либо $n$-граммы $x_{i - (n - 1)} ... x_{i - 1} x_i$ постепенно уменьшается $n$, что позволяет увеличить общность алгоритма и оценить $n$-грамму по частям, но более надёжно. За счёт этого уменьшается вероятность переобучения модели на конкретных данных. \todo{link}

\[ P_n(x_i | x_{i - n + 1}, ..., x_{i - 1}) =
\begin{cases}
	C(x_i | x_{i - n + 1}, ..., x_{i - 1})       & \quad \text{if } C(x_i | x_{i - n + 1}, ..., x_{i - 1}) > k\\
	P_{n - 1}(x_i | x_{i - n + 2}, ..., x_{i - 1})  & \quad \text{otherwise }\\
\end{cases}
\]

\subsection{ Модель Катца (Katz) }

\paragraph{ Обучение модели } Этап обучения модели такой же, как и в случае backoff-модели.

\paragraph{ Применение модели } Модель Катца является улучшенной версией backoff-модели, в которой накладывается динамический дисконт (коэффициенты $d_{w_{i-n+1}...w_i}$ и $\alpha_{w_{i-n+1}...w_{i-1}}$) на оценку $n$-граммы в случае уменьшения $n$. Более подробно о модели Катца можно прочитать в \cite{katz:backoff}.

\[
P_{n}\left( w_i | w_{i-n+1}...w_{i-1} \right) = 
\begin{cases}
d_{w_{i-n+1}...w_i} \dfrac{C(w_{i-n+1}...w_i)}{C(w_{i-n+1}...w_{i-1})} &\text{if $C(w_{i-n+1}...w_i) > k$}\\
\alpha_{w_{i-n+1}...w_{i-1}} P_{n - 1}\left( w_i | w_{i-n+2}...w_{i-1} \right) &\text{otherwise}
\end{cases}
\]

\todo{Рассказать про коэффициенты}

\subsection{ \todo{Maybe Kneser-Ney} }