\section{ Описание эксперимента }\label{sec:experiment}

Для исследования способов исправлять ошибки OCR в тексте необходимо либо иметь большой корпус размеченных данных с результатами распознавания, либо построить некоторую модель частотных ошибок OCR и эмулировать их самостоятельно. Для данной работы был выбран второй подход, поскольку это убирало необходимость интегрироваться с какой-либо OCR машиной (например, ABBYY FineReader). Подобная интеграция могла бы быть достаточно трудоёмкой задачей, не являясь при этом основной целью исследования.

\subsection{ Корпус }

Для обучения и сравнения $n$-граммных моделей использовался корпус html-страниц с ряда японских сайтов (например, www.38shop.jp, www.ewoman.co.jp, www.cat-v.jp и других) общим размером $\approx 8,5 GB$. 

В корпусе было 163244 html-страницы со средним размером $\approx 52 kB$.

Тексты из этого корпуса не были результатом OCR, поэтому в них не должно было быть ошибок, связанных с распознаванием. Эти тексты были признаны верными с точки зрения языка и подходящими для обучения моделей.

В рамках подготовки корпуса к эксперименту тексты были перемешаны, чтобы тематика текста не зависела от его исходного положения в корпусе, из текстов были удалены html-теги, сложное форматирование, небольшое число мусорных символов. Также корпус был единообразно переведён в кодировку Unicode. Подробнее об этих технических этапах -- см. \cref{sec:coding}.

После вышеперечисленных операций корпус был готов к использованию, его размер составлял $\approx 1,7 GB$. При этом размер алфавита в нашем корпусе составлял $\approx 7000$ символов, что в 3 раза меньше размера таблиц Unicode.

Имея данные, готовые к использованию, проанализируем их.

\subsection{ Обработка корпуса }

После обработки корпуса его можно было описать следующими цифрами (\cref{table:corpora}):

\begin{table}[H]
	\begin{center}
\begin{tabular}{|l|c|} \hline
	Параметр & Значение \\ \hline 
	Размер (kB) & 1 721 504 \\
	Символов & 640 604 961 \\
	среди них уникальных & 6 861 \\
	Предложений & 79 497 345	\\  \hline
\end{tabular}
\caption{Характеристики корпуса}
\label{table:corpora}
\end{center}
\end{table}
 
\vspace{20pt}

Если посмотреть на распределение частот отдельных символов (\cref{plt:gramstats1_all}), то оно выглядит так:

\begin{center}

\begin{figure}[H]
\begin{tikzpicture}
	\begin{axis}
		[
		width=12cm, %height=5cm,
		%ybar,
		%bar width=20pt,
		ylabel={Количество, шт},
		xlabel={Порядковый номер символа},
		axis x line=bottom,
		axis y line=left,
		enlarge x limits=0.1,
		enlarge y limits=0.1,
		]	
		\addplot gnuplot[raw gnuplot, mark=none, color=blue]{ plot 'plots/1gramstats.csv' using ($1):($2) every 5 with lines; };
	\end{axis}
\end{tikzpicture}
\caption{Распределение частот униграмм}
\label{plt:gramstats1_all}
\end{figure}
\end{center}

%\begin{figure}[H]
%	\centering
%	\includegraphics{draft.png}
%	\caption{1gramstats}
%	\label{fig:1gramstats_all}
%\end{figure}

Видно, что распределение похоже на обратное экспоненциальное (кстати, это же утверждает закон Ципфа (Zipf, см. \cite{manning})). Проверим эту гипотезу, построив график обратного логарифма (\cref{plt:gramstats1log}):

\pgfplotstableread[col sep = space]{plots/1gramstatslog_small.csv}\loadedtable

\begin{center}

\begin{figure}[H]
	\begin{tikzpicture}
	\begin{axis}
	[
	width=12cm, %height=5cm,
	%ybar,
	%bar width=20pt,
	ylabel={$\log\left( \dfrac{1}{\text{Количество, шт}} \right)$},
	xlabel={Порядковый номер символа},
	axis x line=bottom,
	axis y line=left,
	enlarge x limits=0.1,
	enlarge y limits=0.1,
	minor tick num = 2
	]	
	\addplot gnuplot[raw gnuplot, mark=none, color=blue]{ plot 'plots/1gramstatslog.csv' using ($1):($2) every 5 with lines; };
	\addplot[very thick, red] table [col sep = space, y={create col/linear regression={y=count}}]{\loadedtable};
	\end{axis}
	\end{tikzpicture}
	\caption{Проверка закона Ципфа}
	\label{plt:gramstats1log}
\end{figure}
\end{center}

Действительно, этот график с достаточной точностью ложится на прямую, выбиваясь из неё только в начале списка символов (там находятся самые частотные кандзи, а также практически вся хирагана/катакана, подробнее про распределения различных подмножеств символов см. \cref{sec:noisezipfstats}). Тем самым, эмпирический закон Ципфа был проверен на ещё одном корпусе.

Посмотрев на \cref{plt:gramstats1_all}, можно также заметить, что только очень малая часть символов появляется большое число раз. Посмотрим поближе на "голову"\ того же распределения (\cref{plt:gramstats1_head}):

\begin{center}

\begin{figure}[H]
	\begin{tikzpicture}
	\begin{axis}
	[
	width=12cm, %height=5cm,
	%ybar,
	%bar width=20pt,
	ylabel={Количество, шт},
	xlabel={Порядковый номер символа},
	axis x line=bottom,
	axis y line=left,
	enlarge x limits=0.1,
	enlarge y limits=0.1,
	]	
	\addplot gnuplot[raw gnuplot, mark=none, color=blue]{ plot 'plots/1gramstats.csv' using ($1):($2) every 5::::200 with lines; };
	\end{axis}
	\end{tikzpicture}
	\caption{Частоты униграмм -- голова распределения }
	\label{plt:gramstats1_head}
\end{figure}
\end{center}
Действительно, лишь $\approx 200$ символов встречаются достаточно часто.

Осталюся ещё примерно $6500$ символов, которые входят в алфавит, но статистически мало отличаются от тех символов, что вовсе не встретились в нашем корпусе. Для оптимизации времени работы и занимаемой памяти эти символы можно представить более сжато.

\begin{definition}
	{\textit{Корзина (бакет, bucket)}} -- множество символов, которые считаются статистически малозначимыми и заменяются на U+FFFD (Unicode Replacement Character).
\end{definition}

Бакет $B_i$ характеризуется числом $|\Sigma_{B_i}|$ -- размером алфавита, который остаётся после сливания некоторого хвоста распределения в бакет. Было решено рассматривать бакеты с алфавитами размером $|\Sigma_{B_i}| = \{ 7000, 4800, 2600, 200 \}$, поскольку примерно на эти размеры алфавитов приходятся изменения в характере убывания частот символов. При этом бакет $|\Sigma_{B_i}| = 7000$ представляет собой исходный корпус. 

\textbf{Основным} бакетом впоследствии был выбран $B^* : |\Sigma_{B^*}| = 4800$, поскольку при нём, с одной стороны, реализуется \textbf{сглаживание} хвоста распределения, что косвенно даёт возможность учитывать символы, не встретившиеся в обучающей выборке. С другой стороны, размер алфавита остаётся достаточно большим, что будет важно при рассмотрении шумов (см. \cref{sec:noisezipfstats})

На \cref{fig:bucket_pic} схематично изображено распределение частот после применения бакета с $|\Sigma_{B_i}| = 2600$.

\begin{center}

\begin{figure}[H]
	\begin{tikzpicture}
	\begin{axis}
	[
	width=12cm, %height=5cm,
	%ybar,
	%bar width=20pt,
	ylabel={Количество, шт},
	xlabel={Порядковый номер символа},
	axis x line=bottom,
	axis y line=left,
	enlarge x limits=0.1,
	enlarge y limits=0.1,
	]	
	\addplot gnuplot[raw gnuplot, only marks, color=blue]{ plot 'plots/1gramstats-bucket.csv' using ($1):($2) every 5 with lines; };	\addplot gnuplot[raw gnuplot, only marks, color=red]{ plot 'plots/1gramstats-bucketed.csv' using ($1):($2) every 5 with lines; };
	\end{axis}
	\end{tikzpicture}
	\caption{Распределение частот униграмм: бакет с $|\Sigma_{B_i}| = 2600$}
	\label{fig:bucket_pic}
\end{figure}

\end{center}
Поскольку нам были недоступны корпуса текстов, распознанные какой-либо OCR машиной, было принято решение эмулировать ошибки OCR самим. Это делалось при помощи генератора шума.

\subsection{ Генератор шума и режимы его работы }

\begin{definition}
	{\textit{Шум $Noise = \{ (a_1, a_2), (b_1, b_2, b_3), (c_1, c_2), ... \}$}} -- множество наборов символов алфавита $\Sigma$, которые легко спутать при распознавании. Конкретные шумы определяются эмпирически. 
\end{definition}

Для эмуляции ошибок OCR был разработан скрипт -- генератор шума. Он параметризуется конкретным шумом и частотой его применения.

\begin{definition}
	{\textit{Генератор шума}} -- настраиваемый скрипт, который принимает эталонное предложение $S$, находит в нём символы-представители наборов конкретного шума $x \in S\ |\ \exists \xi = \{ \xi_1, \xi_2, ..., \xi_l \} \in Noise : x \in \xi$, и случайным образом меняет эти символы $x$ на "шумовые" из соответствующего набора $\xi$.
\end{definition}

С помощью шума $Noise$ случайным образом генерируются ошибки в предложениях текста $Text$. Таким образом происходит стохастическая эмуляция ошибок OCR. 

Тестовая часть корпуса была разбита на предложения (см. формальную постановку задачи в \cref{sec:taskdef}), которые независимо друг от друга зашумлялись. Эти предложения после зашумления подавались на вход оценивающему алгоритму $\Theta$, который выбирал лучший из предложенных вариантов.

Были определены следующие шумы, обоснование выбора см. в разделе \cref{sec:taskdef}: 

\begin{itemize}
	\item[\KG] Наборы симвопов, соответствующие добавлению диакритики. Например,
	
	\begin{multicols}{3}
		\begin{CJK}{UTF8}{min}
			かが \\
			きぎ \\
			くぐ \\
			けげ \\
			こご \\
			さざ \\
			しじ \\
			すず \\
			ふぶぷ   \\
			そぞ \\
			ただ \\
			なに \\
			たな \\
			だな \\
			んだ \\
			ちぢ \\
		ほぼぽ \\
	. . .\end{CJK}
	\end{multicols}


	\item[\BS] Большие/маленькие написания букв:
	
	\begin{multicols}{3}
	\begin{CJK}{UTF8}{min}
		あぁ \\
		いぃ\\
		うぅ\\
		えぇ\\
		おぉ\\
		つっ\\
		やゃ\\
		ゆゅ\\
		よょ\\
		わゎ\\
		アァ\\
		イィ\\
		ウゥ\\
		エェ\\
		. . . \end{CJK}
\end{multicols}

	\item[\MX] Комбинация предыдущих режимов.
		\begin{multicols}{3}
		\begin{CJK}{UTF8}{min}
			かが \\
			あぁ \\
			いぃ\\
			うぅ\\
			えぇ\\
			おぉ\\
			ふぶぷ   \\
			そぞ \\
			ただ \\
			なに \\
			たな \\
			だな \\
			んだ \\
			ちぢ \\
			ほぼぽ \\
		. . . \end{CJK}
	\end{multicols}
	
\end{itemize}

Интересно понимать, как выглядит результат работы генератора шума.
Предположим, на вход генератору было дано следующее предложение:

\begin{CJK}{UTF8}{min}キャンペーンは終了致しました。 \end{CJK} 

Тогда для различных шумов и режима "1 символ на предложение" получались такие результаты (\cref{table:noises}), которые затем фиксировались.

\begin{table}[H]
\begin{center}

\begin{tabular}{|c|c|} \hline
	Шум 	& Текст\\ \hline
	Эталон 	& \begin{CJK}{UTF8}{min}キャンペーンは終了致しました。 \end{CJK} \\
	\KG	&  \begin{CJK}{UTF8}{min}ギャンペーン\colorbox{yellow}{\textbf{ぱ}}終了致しました。 \end{CJK} \\
	\BS &  \begin{CJK}{UTF8}{min}キ\colorbox{yellow}{\textbf{ヤ}}ンペーンは終了致しました。 \end{CJK} \\
	\MX 	&  \begin{CJK}{UTF8}{min}\colorbox{yellow}{\textbf{ギ}}ャンペーンは終了致しました。 \end{CJK}  \\ \hline
\end{tabular}
\caption{Различные шумы}
\label{table:noises}
\end{center}
\end{table}

\subsection{ Статистика по шумовым символам }
\label{sec:noisezipfstats}

Посмотрим на то, где в общем распределении символов лежат шумовые символы. На всех графиках этого раздела пунктиром показана граница основного бакета $B^*$ c $|\Sigma_{B^*}| = 4800$.

\paragraph{ \KG } Рассмотрим, как выглядят \KG\ шумовые символы на фоне всех остальных в корпусе (шкала логарифмическая, \cref{plt:kaga-noise}).

\begin{center}

\begin{figure}[H]
	\begin{tikzpicture}
	\begin{axis}
	[
	width=12cm, %height=5cm,
	%ybar,
	%bar width=20pt,
	ylabel={Количество, шт},
	xlabel={Порядковый номер символа},
	axis x line=bottom,
	axis y line=left,
	enlarge x limits=0.1,
	enlarge y limits=0.1,
	xmode=log,
	]	
	\addplot gnuplot[raw gnuplot, mark=none, color=blue, very thick]{ plot 'plots/noise/kaga-no.csv' using ($1):($2) every 5 with filledcurves below x1 ; };
	\addplot gnuplot[raw gnuplot, color=red, only marks]{ plot 'plots/noise/kaga-yes.csv' using ($1):($2) with impulses ; };
	\draw [dashed] (4800, -1000000) -- (4800, 16000000);
	\end{axis}
	\end{tikzpicture}
	\caption{Распределение частот шума \KG}
	\label{plt:kaga-noise}
\end{figure}

\end{center}

Видно, что шумовые символы лежат достаточно равномерно по кривой распределения, что может в будущем помешать оцениванию моделей для небольших размеров алфавита.

\paragraph{ \BS } Приведём аналогичный график для шума, состоящего из больших и маленьких кан (шкала логарифмическая, \cref{plt:bigsmall-noise}).

\begin{center}
	
\begin{figure}[H]
	\begin{tikzpicture}
	\begin{axis}
	[
	width=12cm, %height=5cm,
	%ybar,
	%bar width=20pt,
	ylabel={Количество, шт},
	xlabel={Порядковый номер символа},
	axis x line=bottom,
	axis y line=left,
	enlarge x limits=0.1,
	enlarge y limits=0.1,
	xmode=log,
	]	
	\addplot gnuplot[raw gnuplot, mark=none, color=blue, very thick]{ plot 'plots/noise/bigsmall-no.csv' using ($1):($2) every 5 with filledcurves below x1 ; };
	\addplot gnuplot[raw gnuplot, color=red, only marks]{ plot 'plots/noise/bigsmall-yes.csv' using ($1):($2) with impulses ; };
	\draw [dashed] (4800, -1000000) -- (4800, 16000000);
	\end{axis}
	\end{tikzpicture}
	\caption{Распределение частот шума \BS}
	\label{plt:bigsmall-noise}
\end{figure}

\end{center}

В этом случае видно, что шумовые маленькие буквы в основном сконцентрированы в хвосте распределения. Поэтому для малых размеров алфавита работа с этим шумом бесполезна.

\paragraph{ \MX } Если смешать эти 2 шума (\KG\ и \BS) и построить такой же график для смеси, то результат предсказуемо будет являть собой наложение двух предыдущих графиков (шкала логарифмическая, \cref{plt:mix-noise})

\begin{center}
	
\begin{figure}[H]
	\begin{tikzpicture}
	\begin{axis}
	[
	width=12cm, %height=5cm,
	%ybar,
	%bar width=20pt,
	ylabel={Количество, шт},
	xlabel={Порядковый номер символа},
	axis x line=bottom,
	axis y line=left,
	enlarge x limits=0.1,
	enlarge y limits=0.1,
	xmode=log,
	]	
	\addplot gnuplot[raw gnuplot, mark=none, color=blue, very thick]{ plot 'plots/noise/kaga-no.csv' using ($1):($2) every 5 with filledcurves below x1 ; };
	\addplot gnuplot[raw gnuplot, color=red, only marks]{ plot 'plots/noise/kaga-yes.csv' using ($1):($2) with impulses ; };
	
	\addplot gnuplot[raw gnuplot, color=green, only marks]{ plot 'plots/noise/bigsmall-yes.csv' using ($1):($2) with impulses ; };
	\draw [dashed] (4800, -1000000) -- (4800, 16000000);
	\end{axis}
	\end{tikzpicture}
	\caption{Распределение частот шума \MX}
	\label{plt:mix-noise}
\end{figure}

\end{center}

\subsection{ Ход эксперимента }
\label{sec:confidencemodel}

Глобально эксперимент состоял из следующих этапов:

\begin{enumerate}
	\item Обучение модели:
	
	\begin{enumerate}
		\item Получение $n$-грамм для обучающей выборки;
		
		\item Подсчёт статистики по $n$-граммам;
		
		\item Сериализация статистики на диск;
	\end{enumerate}
	
	\item Применение модели:
	\begin{enumerate}
		\item Десериализация обученной модели;
		
		\item Получение $n$-грамм для тестовой выборки;
		
		\item Оценивание моделью различных вариантов написания для предложения;
		
		\item \label{itm:scoring} Вычисление оценки модели.
	\end{enumerate}	
\end{enumerate}

\begin{definition}
	{\textit{Уверенность (confidence)}} -- порог отношения оценок величины $\dfrac{\max(\xi_1, \xi_2)}{\min(\xi_1, \xi_2)} = Confidence$, при достижении которого оценки $\xi_1, \xi_2$ величины $\xi$ считаются неразличимыми.
\end{definition}

При этом на этапе оценивания вклада результата работы модели на конкретном предложении (\cref{itm:scoring}) существовало 2 альтернативы:

\begin{enumerate}
	\item \label{itm:basescore} $\Theta(S_{good}) > \Theta(S_{noise}) \Rightarrow $ на этом предложении модель отработала хорошо, иначе -- плохо.
	
	\item $\dfrac{\max(\Theta(S_{good}), \Theta(S_{noise}))}{\min(\Theta(S_{good}), \Theta(S_{noise}))} < Confidence \Rightarrow $ на этом предложении модель отработала уверенно, классификация проводится, далее см. \cref{itm:basescore}. Если же различие между оценками меньше порога $Confidence$, то результат -- отказ от классификации этого случая.
	
\end{enumerate}

\subsection{ Baseline эксперимента }

В качестве baseline эксперимента были выбраны результаты работы униграммной модели ($n$-граммная при $n = 1$) (результаты показаны для основного бакета, $|\Sigma_{B^*}| = 4800$). Оценка производилась без учёта уверенности.

Для разных шумов baseline показал следующие результаты (\cref{table:baseline}):

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|} \hline
	Шум 	& Оценка модели \\ \hline
	\KG	& 0.75  \\
	\BS & 0.88  \\
	\MX & 0.76 \\ \hline
\end{tabular}
\caption{Baseline}
\label{table:baseline}
\end{center}
\end{table}