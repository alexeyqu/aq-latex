\section{ Описание эксперимента }\label{sec:experiment}

Для исследования способов исправлять ошибки OCR в тексте необходимо либо иметь большой корпус размеченных данных с результатами распознавания, либо как-то выкручиваться. 

Пришлось выкручиваться, эмулируя ошибки OCR самостоятельно.

\todo{Описание машины -- в приложение}

\subsection{ Корпус }

Для обучения и сравнения $n$-граммных моделей использовался корпус html-страниц с ряда японских сайтов (\todo{Каких? Где?}) общим размером $\approx 8,5 GB$. 

Тексты из этого корпуса не были результатом OCR, поэтому в них не должно было быть ошибок, связанных с распознаванием. Эти тексты были признаны верными с точки зрения языка и подходящими для обучения моделей.

В рамках подготовки корпуса к эксперименту тексты были перемешаны, чтобы тематика текста не зависела от его исходного положения в корпусе, из текстов были удалены html-теги, сложное форматирование, небольшое число мусорных символов. Также корпус был единообразно переведён в кодировку Unicode. Подробнее об этих технических этапах -- см. раздел \ref{sec:coding}.

После вышеперечисленных операций корпус был готов к использованию, его размер составлял $\approx 1,5 GB$. При этом размер алфавита в нашем корпусе составлял $\approx 7000$ символов, что в 3 раза меньше размера таблиц Unicode.

Имея данные, готовые к использованию, было бы глупо не построить по ним несколько графиков.

\subsection{ \todo{Zipf} }

Если посмотреть на распределение частот отдельных символов, то оно выглядело так:

\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{1gramstats}
	\label{fig:1gramstats_all}
\end{figure}

Видно, что распределение похоже на обратно экспоненциальное (кстати, это же утверждает закон Ципфа \todo{link}). Проверим эту гипотезу, построив график обратного логарифма (\todo{формулы}):

\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{zipf}
	\label{fig:zipf1gram}
\end{figure}

Действительно, этот график с достаточной точностью ложится на прямую (\todo{погрешности?}). Тем самым, в NLP закон Ципфа проверен ещё раз.

Посмотрев на Рис. \ref{fig:1gramstats_all}, можно также заметить, что только очень малая часть символов появляется большое число раз. Посмотрим поближе на "голову"\ того же распределения:

\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{1gramstats head}
	\label{fig:gramstats_head}
\end{figure}

Действительно, лишь $\approx 200$ символов встречаются достаточно часто.

Осталюся ещё примерно $6500$ символов, которые входят в алфавит, но статистически мало отличаются от тех символов, что вовсе не встретились в нашем корпусе. Для оптимизации времени работы и занимаемой памяти эти символы можно представить более сжато.

\begin{definition}
	{\textit{Корзина (бакет, bucket)}} -- множество символов, которые считаются статистически малозначимыми и заменяются на U+FFFD (Unicode Replacement Character).
\end{definition}

Бакет $B_i$ характеризуется числом $|\Sigma_{B_i}|$ -- размером алфавита, который остаётся после сливания некоторого хвоста распределения в бакет. Было решено рассматривать бакеты с алфавитами размером $|\Sigma_{B_i}| = \{ 7000, 4800, 2600, 200 \}$, поскольку примерно на эти размеры алфавитов приходятся изменения в характере убывания частот символов.

На Рис. \ref{fig:bucket_pic} схематично изображено распределение частот после применения бакета с $|\Sigma_{B_i}| = 200$.

\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{bucket}
	\label{fig:bucket_pic}
\end{figure}

Поскольку нам были недоступны корпуса текстов, распознанные какой-либо OCR машиной, было принято решение эмулировать ошибки OCR самим. Это делалось при помощи генератора шума.

\subsection{ Генератор шума и режимы его работы }

\begin{definition}
	{\textit{Шум $Noise = \{ (a_1, a_2), (b_1, b_2, b_3), (c_1, c_2), ... \}$}} -- множество наборов символов алфавита $\Sigma$, которые легко спутать при распознавании. Конкретные шумы определяются эмпирически. 
\end{definition}

Для эмуляции ошибок OCR был разработан скрипт -- генератор шума. Он параметризуется конкретным шумом и частотой его применения.

\begin{definition}
	{\textit{Генератор шума}} -- настраиваемый скрипт, который принимает эталонное предложение $S$, находит в нём символы-представители наборов конкретного шума $x \in S\ |\ \exists \xi = \{ \xi_1, \xi_2, ..., \xi_l \} \in Noise : x \in \xi$, и случайным образом меняет эти символы $x$ на "шумовые" из соответствующего набора $\xi$.
\end{definition}

С помощью шума $Noise$ случайным образом генерируются ошибки в предложениях текста $Text$. Таким образом происходит стохастическая эмуляция ошибок OCR. 

Тестовая часть корпуса была разбита на предложения (см. формальную постановку задачи в разделе \ref{sec:taskdef}), которые независимо друг от друга зашумлялись. Эти предложения после зашумления подавались на вход оценивающему алгоритму $\Theta$, который выбирал лучший из предложенных вариантов.

Были определены следующие шумы, обоснование выбора см. в разделе \ref{sec:taskdef}: 

\begin{itemize}
	\item[KaGa] Наборы симвопов, соответствующие добавлению диакритики. Например,
	
	\begin{multicols}{3}
		\begin{CJK}{UTF8}{min}
			かが \\
			きぎ \\
			くぐ \\
			けげ \\
			こご \\
			さざ \\
			しじ \\
			すず \\
			ふぶぷ   \\
			そぞ \\
			ただ \\
			なに \\
			たな \\
			だな \\
			んだ \\
			ちぢ \\
			つづ \\
		ほぼぽ \end{CJK}
	\end{multicols}
	
	\item[HalfWidth] Полуширинные/полноширинная катакана:
	
		\begin{multicols}{3}
		\begin{CJK}{UTF8}{min}
				ｦヲ\\
			ｧァ\\
			ｨィ\\
			ｩゥ\\
			ｪェ\\
			ｫォ\\
			ｬャ\\
			ｭュ\\
			ｮョ\\
			ｯッ\\
			ｰー\\
			ｱア\\
			ｲイ\\
			ｳウ\\
			ｴエ\\
			ｵオ\\
			ｶカ\\
			ｷキ  \end{CJK}
	\end{multicols}
	


	\item[BigSmall] Большие/маленькие написания букв:
	
	\begin{multicols}{3}
	\begin{CJK}{UTF8}{min}
		あぁ \\
		いぃ\\
		うぅ\\
		えぇ\\
		おぉ\\
		つっ\\
		やゃ\\
		ゆゅ\\
		よょ\\
		わゎ\\
		アァ\\
		イィ\\
		ウゥ\\
		エェ\\
		オォ \end{CJK}
\end{multicols}

	\item[Mix] Комбинация предыдущих режимов.
		\begin{multicols}{3}
		\begin{CJK}{UTF8}{min}
			かが \\
			きぎ \\
			くぐ \\
			けげ \\
			こご \\
			ｬャ\\
			ｭュ\\
			ｮョ\\
			ｯッ\\
			ｰー\\
			ｱア\\
			アァ\\
			イィ\\
			ウゥ\\
			エェ\\
			オォ \end{CJK}
	\end{multicols}
	
\end{itemize}

\todo{ Срез шума в Ципфе -- график }

Интересно понимать, как выглядит результат работы генератора шума.
Предположим, на вход генератору было дано следующее предложение:

\begin{CJK}{UTF8}{min}キャンペーンは終了致しました。 \end{CJK} 

Тогда для различных шумов и режима "1 символ на предложение" получались такие результаты, которые затем фиксировались.

\begin{tabular}{c|c}
	Шум 	& Текст (пер. с яп. Campaign has ended)\\
	Эталон 	& \begin{CJK}{UTF8}{min}キャンペーンは終了致しました。 \end{CJK} \\
	KaGa	&  \begin{CJK}{UTF8}{min}ギャンペーン\colorbox{yellow}{\textbf{ぱ}}終了致しました。 \end{CJK} \\
	HalfWidth &  \begin{CJK}{UTF8}{min}キャンペ\colorbox{yellow}{\textbf{ｰ}}ンは終了致しました。 \end{CJK} \\
	BigSmall &  \begin{CJK}{UTF8}{min}キ\colorbox{yellow}{\textbf{ヤ}}ンペーンは終了致しました。 \end{CJK} \\
	Mix 	&  \begin{CJK}{UTF8}{min}\colorbox{yellow}{\textbf{ギ}}ャンペーンは終了致しました。 \end{CJK} 
\end{tabular}

\subsection{ Baseline эксперимента }

В качестве бейзлайна эксперимента была выбрана униграммная модель ($n = 1$).

Для разных шумов она показала следующие результаты, $|\Sigma_{B_i}| = 4800$:

\begin{tabular}{c|c}
	Шум 	& Оценка модели \\
	KaGa	& 0.70  \\
	HalfWidth &  0.71 \\
	BigSmall & 0.77  \\
	Mix 	&  
\end{tabular}

\todo{достать и добавить результаты для остальных бакетов}

\newpage
\section{ Реализация модели }\label{sec:coding}

\subsection{ Общее окружение: nltk, pygtrie }

\subsection{ Полезные утилиты }

\todo{ pickle }

\todo{ UnicodeDammit }

\todo{ dot }

\subsection{ Пример работы и статистик }

\todo{ Разобрать предложение и пройтись по этапам визуализации результата (до svg-картинки с траем). }
