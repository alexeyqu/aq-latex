\documentclass[14pt,russian]{extreport}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
%\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\geometry{verbose,tmargin=2.0cm,bmargin=2.0cm,lmargin=3.0cm,rmargin=2.0cm}
%\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}

\usepackage{amssymb}
\usepackage{setspace}
\usepackage{babel}
\usepackage{color}
\usepackage[hidelinks]{hyperref}
\usepackage{natbib}
%\usepackage{authordate1-4}
%\bibliographystyle{authordate1}
\bibliographystyle{unsrt}
\usepackage{amsmath}
\newtheorem{hyp}{Гипотеза}
\usepackage[draft]{graphicx}
\graphicspath{img/}
\usepackage{float}
\usepackage{placeins}
\usepackage{chngcntr}
\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}
\usepackage{multicol}

\usepackage{CJKutf8}
%\onehalfspacing
\linespread{1.5}

% remove chapter number from section headings
\renewcommand*\thesection{\arabic{section}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand*{\argminl}{\argmin\limits}
\newcommand*{\argmaxl}{\argmax\limits}

% rename chapters
\makeatletter
\renewcommand{\@chapapp}{Глава}
\makeatother

% continuous equation numbering through chapters
\usepackage{chngcntr}
\counterwithout{equation}{chapter}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Определение}[subsection]

\usepackage{indentfirst}
\frenchspacing

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\begin{document}
	
\include{titlepage}
\onehalfspacing

\addtocounter{page}{1}
\tableofcontents{}

\newpage
\section*{Введение}
\addcontentsline{toc}{section}{Введение}

История попыток распознать текст началась более века назад. В 1914 году Эмануэль Гольдберг разработал устройство, которой считывало символы и транслировало их в телеграфный код. Примерно в то же время ирландский химик Эдмунд Фурнье д’Альбе создал и запатентовал «оптофон» — прибор, умеющий переводить написанное в систему звуков, различающихся по высоте. Оптофон предназначался для того, чтобы слепые могли «читать».

В 1929 году Густав Таушек (Gustav Tauschek) разработал метод оптического распознавания текста. Машина Таушека представляла собой механическое устройство, которое использовало шрифтовые шаблоны и фотодетектор. Он запатентовал своё изобретение сначала в Германии, а позднее и в США, в 1935 году. Это и положило начало проблеме качественного оптического распознавания символов (Optical Character Recognition, OCR).

Коммерческое производство подобных маних было налажено уже в 1950-х, после войны. Использовавшие наработки военных, производители OCR-машин продвигались всё дальше, увеличивая применимость технологии и качество распознавания.

Постепенно появлялись как универсальные OCR-программы (ABBYY FineReader, Adobe Acrobat), так и специализированные для конкретной области (SmartScore для нотной записи, Persian Reader для фарси и т.д.). При этом точность в задаче распознавания напечатанных латинских символов достигла 99\%-100\% качества, в то время как корректное распознавание рукописного текста или текста, написанного в другом алфавите, до сих пор является темой множества исследований. Особняком стоит задача распознавания текста на восточных языках (китайский, японский, корейский, ...), из-за большого размера алфавита в этих языках.

Настоящая работа представляет собой сравнение некоторых методов машинного обучения для исправления ошибок распознавания текста в японском языке. 

Спектр способов, которыми можно решать проблему автоматического исправления ошибок, довольно широк, и включает в себя различные вариации $n$-граммных методов ($n$-gram models), использование нейросетей (Neural Networks, NN), скрытых моделей Маркова (Hidden Markov Models, HMM) и прочих методов машинного обучения. Более подробный обзор основных современных подходов можно найти в \cite{das:survey}.
 
Среди возможных решений использование $n$-граммных моделей занимает особую нишу из-за относительной прозрачности и интуитивности принципов работы, и в то же время достаточно широких возможностей по настройке алгоритма.

Подход, предложенный в \cite{nagata:shape}, использует $n$-граммные модели, а также различные алгоритмы сглаживания для исправления опечаток, опираясь на словное деление текста. 

В работе \cite{nagata:context} также даются эвристики для определения границ слов, использующие граф линейного деления (ГЛД). Эти границы слов затем используются в $n$-граммной модели в качестве вспомогательного контекста.

Более подробно эти и другие подходы разобраны в соответствующем разделе (\ref{sec:litreview}).

Данное исследование призвано рассмотреть некоторые из $n$-граммных моделей и сравнить их эффективность в задаче исправления опечаток в японском языке.

Актуальным приложением этой работы является система распознавания восточных языков в ABBYY FineReader.

\newpage
\include{taskdef}

\newpage
\include{litreview}

\newpage
\include{models}

\newpage
\include{experiment}

\newpage
\include{results}

\newpage
\include{analysis}

\newpage
\section{ Заключение }\label{sec:epilogue}

\newpage
\include{bib}

\end{document}
