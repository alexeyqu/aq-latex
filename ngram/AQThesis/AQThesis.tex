\documentclass[14pt,russian]{extreport}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
%\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\geometry{verbose,tmargin=2.0cm,bmargin=2.0cm,lmargin=3.0cm,rmargin=2.0cm}
%\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}

\usepackage{amssymb}
\usepackage{setspace}
\usepackage{babel}
\usepackage{color}
\usepackage[hidelinks]{hyperref}
\usepackage{natbib}
%\usepackage{authordate1-4}
%\bibliographystyle{authordate1}
\bibliographystyle{unsrt}
\usepackage{amsmath}
\newtheorem{hyp}{Гипотеза}
\usepackage[draft]{graphicx}
\graphicspath{img/}
\usepackage{float}
\usepackage{placeins}
\usepackage{chngcntr}
\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}

%\onehalfspacing
\linespread{1.5}

% remove chapter number from section headings
\renewcommand*\thesection{\arabic{section}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand*{\argminl}{\argmin\limits}
\newcommand*{\argmaxl}{\argmax\limits}

% rename chapters
\makeatletter
\renewcommand{\@chapapp}{Глава}
\makeatother

% continuous equation numbering through chapters
\usepackage{chngcntr}
\counterwithout{equation}{chapter}

\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Определение}[subsection]

\usepackage{indentfirst}
\frenchspacing

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\begin{document}
	
\include{titlepage}
\onehalfspacing

\addtocounter{page}{1}
\tableofcontents{}

\newpage
\section*{Введение}
\addcontentsline{toc}{section}{Введение}

История попыток распознать текст началась более века назад. В 1914 году Эмануэль Гольдберг разработал устройство, которой считывало символы и транслировало их в телеграфный код. Примерно в то же время ирландский химик Эдмунд Фурнье д’Альбе создал и запатентовал «оптофон» — прибор, умеющий переводить написанное в систему звуков, различающихся по высоте. Оптофон предназначался для того, чтобы слепые могли «читать».

В 1929 году Густав Таушек (Gustav Tauschek) разработал метод оптического распознавания текста. Машина Таушека представляла собой механическое устройство, которое использовало шрифтовые шаблоны и фотодетектор. Он запатентовал своё изобретение сначала в Германии, а позднее и в США, в 1935 году. Это и положило начало проблеме качественного оптического распознавания символов (Optical Character Recognition, OCR).

Коммерческое производство подобных маних было налажено уже в 1950-х, после войны. Использовавшие наработки военных, производители OCR-машин продвигались всё дальше, увеличивая применимость технологии и качество распознавания.

Постепенно появлялись как универсальные OCR-программы (ABBYY FineReader, Adobe Acrobat), так и специализированные для конкретной области (SmartScore для нотной записи, Persian Reader для фарси и т.д.). При этом точность в задаче распознавания напечатанных латинских символов достигла 99\%-100\% качества, в то время как корректное распознавание рукописного текста или текста, написанного в другом алфавите, до сих пор является темой множества исследований. Особняком стоит задача распознавания текста на восточных языках (китайский, японский, корейский, ...), из-за большого размера алфавита в этих языках.

Настоящая работа представляет собой сравнение некоторых методов машинного обучения для исправления ошибок распознавания текста в японском языке. 

Спектр способов, которыми можно решать проблему автоматического исправления ошибок, довольно широк, и включает в себя различные вариации $n$-граммных методов ($n$-gram models), использование нейросетей (Neural Networks, NN), скрытых моделей Маркова (Hidden Markov Models, HMM) и прочих методов машинного обучения. Более подробный обзор основных современных подходов можно найти в \cite{das:survey}.
 
Среди возможных решений использование $n$-граммных моделей занимает особую нишу из-за относительной прозрачности и интуитивности принципов работы, и в то же время достаточно широких возможностей по настройке алгоритма.

Подход, предложенный в \cite{nagata:shape}, использует $n$-граммные модели, а также различные алгоритмы сглаживания для исправления опечаток, опираясь на словное деление текста. 

В работе \cite{nagata:context} также даются эвристики для определения границ слов, использующие граф линейного деления (ГЛД). Эти границы слов затем используются в $n$-граммной модели в качестве вспомогательного контекста.

Более подробно эти и другие подходы разобраны в соответствующем разделе (\ref{sec:litreview}).

Данное исследование призвано рассмотреть некоторые из $n$-граммных моделей и сравнить их эффективность в задаче исправления опечаток в японском языке.

Актуальным приложением этой работы является система распознавания восточных языков в ABBYY FineReader.

\section{ Постановка задачи }\label{sec:taskdef}

\begin{definition}{\textit{Оптическое распознавание символов (Optical Character Recognition, OCR)}} -- процесс считывания текста с физического носителя и его сохранения в цифровом формате. Текст состоит из \textit{символов}.
\end{definition}

\begin{definition}{\textit{Ошибка OCR}} -- случай, когда очередной символ текста распознался неверно или не распознался. Ведёт к понижению качества распознавания.
\end{definition}

\begin{definition}{\textit{$N$-грамма}} -- последовательность из $n$ элементов (слов, звуков, символов). Анализируя их частотности, можно строить модели для анализа и синтеза языка.
\end{definition}

\begin{definition}{\textit{$N$-граммная модель}} -- вероятностная модель языка, которая рассчитывает вероятность последнего элемента $n$-граммы, если известны все предыдущие. \\
При использовании $n$-граммных моделей предполагается, что появление каждого элемента зависит только от предыдущих элементов.
\end{definition}

\textbf{Цель работы} -- сравнить эффективность различных символьных $n$-граммных моделей в задаче исправления ошибок OCR в японском языке.

Из цели работы вытекают следующие \textbf{задачи}:
\begin{itemize}
	\item Рассмотреть существующие подходы к $n$-граммному моделированию японского языка;
	
	\item Реализовать некоторые модели;
	
	\item Развернуть систему для тестирования и сравнения моделей.
\end{itemize}

Чтобы понять специфику цели работы, нужно учесть особенности японского языка.

Очевидно, что устройство японского языка на уровне конкретных символов сложнее, чем устройство языков латино-романской группы (в которых существует всего 25-40 символов, учитывая возможную диакритику).

\subsection{ Обзор японского языка }

Письменный японский текст -- это комбинация слогово-фонетических символов (кана) и иероглифов (кандзи).
Слоговая азбука кана делится на катакану и хирагану, которые представляют собой разные графические формы одних и тех же слогов. \todo{сказать, почему забиваем на окуригану и т.д.}

Рассмотрим эти символы подробнее:

\begin{itemize}
	\item Хирагана (см. Рис. \ref{fig:hirag_sample}), символы более округлые, чем в катакане. В основном используется для образования грамматических морфем.
	\begin{figure}[H]
		\centering
		\includegraphics{hirag_sample.png}
		\caption{hirag\_sample}
		\label{fig:hirag_sample}
	\end{figure}
	
	\item Хирагана (см. Рис. \ref{fig:katak_sample}), символы более резкие, чем в хирагане. Используется для транскрибирования иностранных заимствованных слов \todo{примеры :)}.
	\begin{figure}[H]
		\centering
		\includegraphics{katak_sample.png}
		\caption{katak\_sample}
		\label{fig:katak_sample}
	\end{figure}

	\item Также есть диакритические символы -- дакутен, хандакутен (\todo{?}) (см. Рис. \ref{fig:dakut_sample}). Они могут применяться как к катакане, так и к хирагане, и определённым образом влияют на звучание слогов.
	\begin{figure}[H]
		\centering
		\includegraphics{draft.png}
		\caption{dakut\_sample}
		\label{fig:dakut_sample}
	\end{figure}

	\item Кандзи (см. Рис. \ref{fig:kandji_sample}). Это символы, несущие семантическую нагрузку. С точки зрения написания иероглифы можно поделить на пиктограммы, идеограммы и фонограммы \todo{?}. 
	\begin{figure}[H]
		\centering
		\includegraphics{draft.png}
		\caption{kandji\_sample}
		\label{fig:kandji_sample}
	\end{figure}
\end{itemize}

Кана различает 46 слогов, которые могут записываться как катаканой, так и хираганой. А вот иероглифов кандзи существует гораздо больше (6000 достаточно для жизни, а стандарт Unicode определяет 21000) \todo{цифры}.

Японский текст записывается с помощью комбинаций кандзи, кан и пунктуации, при этом отсутствует пробельное деление предложений на слова (см. Рис. \ref{fig:japtext_sample}).
	\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{japtext}
	\label{fig:japtext_sample}
\end{figure}

По сравнению с латино-романскими языками, где алфавит меньше в сотни раз, а деление текста на слова очевидно, задача корректного распознавания символов становится значительно сложнее. Это требует более изощрённых подходов для автоматического анализа распознанного текста и поиска ошибок в нём.

Рассмотрим несколько примеров символов, которые легко спутать.

\subsection{ Путающиеся символы в японском }

\begin{itemize}
	\item[2Kana] 2 похожие каны. Таким случаев достаточно мало, а методы их различения уже существуют.
	\begin{figure}[H]
		\center{\includegraphics[scale=1.0]{KanaO.png}\ и \includegraphics[scale=1.0]{KanaWi.png}}
	\end{figure}

	\item[KaGa] Кана может легко путаться с соответствующим её дакутен-символом.
	\begin{figure}[H]
		\center{\includegraphics[scale=1.0]{draft.png}\ и \includegraphics[scale=1.0]{draft.png}}
	\end{figure}

	\item[BigSmall] Существуют большие и маленькие каны, которые нужно различать.
	\begin{figure}[H]
		\center{\includegraphics[scale=1.0]{draft.png}\ и \includegraphics[scale=1.0]{draft.png}}
	\end{figure}

	\todo{HalfWidth?}
	
\end{itemize}

\subsection{ Формальная постановка задачи }

\begin{definition}
	{\textit{Алфавит $\Sigma = \{ a, b, c, .. \}$}} -- множество символов в данном языке. В японском языке их около 80000, стандарт Unicode поддерживает примерно 21000.
\end{definition}

\begin{definition}
	{\textit{Текст $Text \in \Sigma^+$}} -- последовательность символов из алфавита $\Sigma$ положительной длины.
\end{definition}

\begin{definition}
	Текст делится на конечное множество {\textit{предложений $S = \{ S_1, S_2, S_3, ... \}$}} знаками пунктуации и форматированием. $Text = S_1S_2S_3...$.
\end{definition}

Для каждого из предложения текста существует единственно верный вариант написания \todo{а что делаем с омонимией?}, а также некоторое (фиксированное) число неверных. Требуется ответить, какой из вариантов верен.

\begin{definition}
	{\textit{Оценивающий алгоритм (estimator) $\Theta : S \rightarrow \mathbb{R}^+ $}} -- функция, возвращающая оценку правильности варианта $S$.
\end{definition}

Среди $k$ вариантов предложения выбирается наилучший: $S_{best} = \argmaxl_{S} \Theta(S)$, который и считается правильным.

Если $S_{best}$ угадано верно, то на данном предложении алгоритм $\Theta$ отработал правильно.

\begin{definition}
	{\textit{Качество алгоритма $Q(\Theta) = \dfrac{\#\{ \text{угаданных предложений} \}}{\#\{ \text{всего предложений} \}}$}}.
\end{definition}

Задача -- реализовать ряд оценивающих алгоритмов (см. раздел \ref{sec:models}), основанных на $n$-граммных моделях, и сравнить их по качеству.

\newpage
\section{ Обзор источников }\label{sec:litreview}

\todo{include{litreview}}

\begin{itemize}
	\item Banerjee. 

	\item Nagata - actual

	\item Nagata - old
\end{itemize}

\newpage
\section{ Описание моделей оценивания текста }\label{sec:models}

Перед обучением моделей корпус разбивается на независимые и гомогенные части: обучающую и тестовую выборки. Обучающая выборка используется для обучения модели, тестовая -- для проверки качества обучения и, собственно, оценки модели.

В работе рассматриваются следующие модели: 

\begin{itemize}
	\item $n$-граммные с фиксированным $n,\ n \in \{1,2,3\}$
	
	\item Backoff-модель, $n_{max} \in \{ 3, 5, 7 \}$
	
	\item Модель Катца (Katz),  $n_{max} \in \{ 3, 5, 7 \}$
\end{itemize}

Также из-за большого размера алфавита необходимо использовать сглаживание (smoothing) для учёта символов и $n$-грамм, не встретившихся в обучающей выборке. Подробнее о механизме сглаживания -- см. раздел \ref{sec:experiment}.

\subsection{ $N$-граммные модели с фиксированным $n$ }

\paragraph{ Обучение модели } Для данного $n$ по обучающей выборке собираются статистики по всем $n$-граммам. Эти статистики затем нормализуются и сериализуются для дальнейшего использования.

$$ C(x_{i - n + 1}, ..., x_{i - 1}, x_i) $$ 

\paragraph{ Применение модели } В силу простоты модели оценка $n$-граммы из тестовой выборки берётся напрямую из собранных на предыдущем этапе статистик.

$$ P(x_i | x_{i - n + 1}, ..., x_{i - 1}) = C(x_{i - n + 1}, ..., x_{i - 1}, x_i) $$

\subsection{ Backoff-модель } 

\paragraph{ Обучение модели } Этап обучения модели практически такой же, как и в случае простой $n$-граммной модели, с разницей в том, что здесь собираются статистики для всех $n \leq n_{max}$.

\paragraph{ Применение модели } Идея backoff-подхода состоит в том, что при нехватке данных для оценки какой-либо $n$-граммы $x_{i - (n - 1)} ... x_{i - 1} x_i$ постепенно уменьшается $n$, что позволяет увеличить общность алгоритма и оценить $n$-грамму по частям, но более надёжно. За счёт этого уменьшается вероятность переобучения модели на конкретных данных. \todo{link}

\[ P_n(x_i | x_{i - n + 1}, ..., x_{i - 1}) =
\begin{cases}
	C(x_i | x_{i - n + 1}, ..., x_{i - 1})       & \quad \text{if } C(x_i | x_{i - n + 1}, ..., x_{i - 1}) > k\\
	P_{n - 1}(x_i | x_{i - n + 2}, ..., x_{i - 1})  & \quad \text{otherwise }\\
\end{cases}
\]

\subsection{ Модель Катца (Katz) }

\paragraph{ Обучение модели } Этап обучения модели такой же, как и в случае backoff-модели.

\paragraph{ Применение модели } Модель Катца является улучшенной версией backoff-модели, в которой накладывается динамический дисконт (коэффициенты $d_{w_{i-n+1}...w_i}$ и $\alpha_{w_{i-n+1}...w_{i-1}}$) на оценку $n$-граммы в случае уменьшения $n$. Более подробно о модели Катца можно прочитать в \todo{link}

\[
P_{n}\left( w_i | w_{i-n+1}...w_{i-1} \right) = 
\begin{cases}
d_{w_{i-n+1}...w_i} \dfrac{C(w_{i-n+1}...w_i)}{C(w_{i-n+1}...w_{i-1})} &\text{if $C(w_{i-n+1}...w_i) > k$}\\
\alpha_{w_{i-n+1}...w_{i-1}} P_{n - 1}\left( w_i | w_{i-n+2}...w_{i-1} \right) &\text{otherwise}
\end{cases}
\]

\todo{Рассказать про коэффициенты}

\subsection{ \todo{Maybe Kneser-Ney} }

\newpage
\section{ Описание эксперимента }\label{sec:experiment}

Для исследования способов исправлять ошибки OCR в тексте необходимо либо иметь большой корпус размеченных данных с результатами распознавания, либо как-то выкручиваться. 

Пришлось выкручиваться, эмулируя ошибки OCR самостоятельно.

\subsection{ \todo{Описание машины -- надо ли?} }

\subsection{ Корпус }

Для обучения и сравнения $n$-граммных моделей использовался корпус html-страниц с ряда японских сайтов (\todo{Каких? Где?}) общим размером $\approx 8,5 GB$. 

Тексты из этого корпуса не были результатом OCR, поэтому в них не должно было быть ошибок, связанных с распознаванием. Эти тексты были признаны верными с точки зрения языка и подходящими для обучения моделей.

В рамках подготовки корпуса к эксперименту тексты были перемешаны, чтобы тематика текста не зависела от его исходного положения в корпусе, из текстов были удалены html-теги, сложное форматирование, небольшое число мусорных символов. Также корпус был единообразно переведён в кодировку Unicode. Подробнее об этих технических этапах -- см. раздел \ref{sec:coding}.

После вышеперечисленных операций корпус был готов к использованию, его размер составлял $\approx 1,5 GB$. При этом размер алфавита в нашем корпусе составлял $\approx 7000$ символов, что в 3 раза меньше размера таблиц Unicode.

Имея данные, готовые к использованию, было бы глупо не построить по ним несколько графиков.

\subsection{ \todo{Zipf} }

Если посмотреть на распределение частот отдельных символов, то оно выглядело так:

\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{1gramstats}
	\label{fig:1gramstats_all}
\end{figure}

Видно, что распределение похоже на обратно экспоненциальное (кстати, это же утверждает закон Ципфа \todo{link}). Проверим эту гипотезу, построив график обратного логарифма (\todo{формулы}):

\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{zipf}
	\label{fig:zipf1gram}
\end{figure}

Действительно, этот график с достаточной точностью ложится на прямую (\todo{погрешности?}). Тем самым, в NLP закон Ципфа проверен ещё раз.

Посмотрев на Рис. \ref{fig:1gramstats_all}, можно также заметить, что только очень малая часть символов появляется большое число раз. Посмотрим поближе на "голову"\ того же распределения:

\begin{figure}[H]
	\centering
	\includegraphics{draft.png}
	\caption{1gramstats head}
	\label{fig:1gramstats_head}
\end{figure}

Действительно, лишь $\approx 200$ символов встречаются достаточно часто.

Осталюся ещё примерно $6500$ символов, которые входят в алфавит, но статистически мало отличаются от тех символов, что вовсе не встретились в нашем корпусе. Для оптимизации времени работы и занимаемой памяти эти символы можно представить более сжато.

\begin{definition}
	{\textit{Корзина (бакет, bucket)}} -- множество символов, которые считаются статистически малозначимыми и заменяются на U+FFFD (Unicode Replacement Character).
\end{definition}

Бакет $B_i$ характеризуется числом $|\Sigma_{B_i}|$ -- размером алфавита, который остаётся после сливания некоторого хвоста распределения в бакет. Было решено рассматривать бакеты с алфавитами размером $|\Sigma_{B_i}| = \{  \}$

\subsection{ Генератор шума и режимы его работы }

\begin{definition}
	{\textit{Шум $Noise = \{ (a_1, a_2), (b_1, b_2, b_3), (c_1, c_2), ... \}$}} -- множество наборов символов алфавита $\Sigma$, которые легко спутать при распознавании. Конкретные шумы определяются эмпирически. 
\end{definition}

С помощью шума $Noise$ случайным образом генерируются ошибки в предложениях текста $Text$. Таким образом происходит стохастическая эмуляция ошибок OCR.

Поскольку нам были недоступны корпуса текстов, распознанные какой-либо OCR машиной, было принято решение симулировать ошибки OCR самим. Тестовая часть корпуса была разбита на предложения (см. формальную постановку задачи), которые независимо друг от друга зашумлялись.

Для данного шума ( Ka-Ga, HalfWidth, BigSmall, Mix ) выбирались индексы символов в предложении, которые затем случайно менялись на какой-то из своего множества.

Были определены следующие шумы: 

\todo{Примеры}

\begin{itemize}
	\item KaGa 

	\item HalfWidth

	\item BigSmall

	\item Mix
\end{itemize}

\todo{Примеры зашумлённого текста}

\subsection{ Baseline }

\todo{Вспомнить про Baseline, и как мы его считали.}

\subsection{ Пример работы и статистик }

\todo{ Разобрать предложение и пройтись по этапам визуализации результата (до svg-картинки с траем). }

\newpage
\section{ Реализация модели }\label{sec:coding}

\subsection{ nltk, Python, избранные куски кода в приложение? }

\newpage
\section{ Результаты эксперимента }\label{sec:results}

Сравнение моделей для различных шумов, циферки.

\newpage
\section{ Анализ результатов }\label{sec:analysis}

\newpage
\section{ Заключение }\label{sec:epilogue}

\newpage
\addcontentsline{toc}{section}{Список литературы}
\begin{thebibliography}{00}
	
\bibitem{das:survey} 
\textit{Soumendu Das et al.} Survey of Pattern Recognition Approaches in Japanese Character Recognition //\\
(IJCSIT) International Journal of Computer Science and Information Technologies, Vol. 5(1), 2014. P.~93~-~99.

\bibitem{nagata:shape} 
\textit{Nagata, Masaaki} Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model //\\
Proceedings of the 17th International Conference on Computational Linguistics - Volume 2, 1998. P.~922~-~928.

\bibitem{nagata:context} 
\textit{Nagata, Masaaki} Context-based Spelling Correction for Japanese OCR //\\
Proceedings of the 16th Conference on Computational Linguistics - Volume 2, 1996. P.~806~-~811.

\bibitem{python:pygtrie} Pygtrie documentation [Электронный ресурс]. \\ URL: http://pygtrie.readthedocs.io/en/latest/ -- 2014

\end{thebibliography}

\end{document}
